services:
  # server:
  #   container_name: openai-server
  #   depends_on:
  #     - registry
  #   build:
  #     context: .
  #     target: final
  #     args:
  #       APP_NAME: "openai-server"
  #   network_mode: host
  #   command: ["/bin/server", "--http-bind", "0.0.0.0:18888", "--control-bind", "0.0.0.0:28888", "--registry-server", "ws://openai-registry:3000/ws"]
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]
  client:
    container_name: openai-client
    restart: always
    build:
      context: ./bin/desktop-app/view
    ports:
      - 80:5173
    environment:
      - VITE_VLLM_URL="https://api.llm.wavefy.network"
      - VITE_VLLM_CONTROLS_URL="https://control.llm.wavefy.network"
      - VITE_VLLM_CONTRACT="0xf24a5c56cbaac7756413042d3456bef5e9aa6235defa675df962b7e541f1d20b"
      - VITE_NODIT_GQL_API="https://aptos-testnet.nodit.io/c4SvS8ZuP7dI9kHC2dRBpFzSKjmFS2eb/v1/graphql"
  # registry:
  #   container_name: openai-registry
  #   build:
  #     context: .
  #     target: final
  #     args:
  #       APP_NAME: "registry-server"
  #   ports:
  #     - 3000:3000
  #   restart: always
  
  caddy:
    container_name: openai-caddy
    image: caddy:2
    ports:
      - 80:80
      - 443:443
    volumes:
      - ./Caddyfile:/etc/caddy/Caddyfile
      - caddy_data:/data
      - caddy_config:/config
    # depends_on:
    #   - server
    #   - client
    #   - registry
    command: ["caddy", "run", "--config", "/etc/caddy/Caddyfile", "--adapter", "caddyfile"]
    restart: always

volumes:
  caddy_data:
  caddy_config: